"""
QUICK FIX - Generate synthetic AI voices and retrain model
This script will:
1. Generate AI voices using gTTS (Google Text-to-Speech)
2. Balance the dataset
3. Retrain the model with proper class separation

This ensures human voices are properly distinguished from AI voices.
"""

import os
import sys
import numpy as np
import pickle
import glob
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

from audio_processor import AudioProcessor
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

class QuickModelFixer:
    def __init__(self):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_dir = os.path.join(self.base_dir, 'data', 'train')
        self.output_dir = os.path.join(self.base_dir, 'backend', 'models')
        self.audio_processor = AudioProcessor()
        os.makedirs(self.output_dir, exist_ok=True)
    
    def generate_ai_voices(self):
        """Generate synthetic AI voices using gTTS"""
        print("\n" + "="*70)
        print("üé§ GENERATING SYNTHETIC AI VOICES")
        print("="*70)
        
        try:
            from gtts import gTTS
            print("\n‚úì gTTS available, generating AI voices...")
        except ImportError:
            print("\nüì¶ Installing gTTS...")
            os.system(f"{sys.executable} -m pip install -q gtts")
            from gtts import gTTS
        
        ai_dir = os.path.join(self.data_dir, 'ai_generated')
        os.makedirs(ai_dir, exist_ok=True)
        
        # Sample texts for AI voice generation
        texts = [
            "Hello world, this is an AI voice.",
            "Artificial intelligence is changing the world.",
            "This is a synthesized voice generated by text to speech technology.",
            "Machine learning models can detect AI voices accurately.",
            "Voice authentication is becoming increasingly important.",
            "Natural language processing helps computers understand speech.",
            "Deep learning networks improve voice quality significantly.",
            "Voice detection systems must be trained on diverse data.",
            "The future of AI includes better voice synthesis methods.",
            "Testing and validation are crucial for AI systems.",
        ]
        
        generated_count = 0
        
        for i, text in enumerate(texts, 1):
            try:
                # Generate voice
                tts = gTTS(text=text, lang='en', slow=False)
                
                # Save audio
                filepath = os.path.join(ai_dir, f'ai_voice_{i:03d}.wav')
                tts.save(filepath)
                
                print(f"   ‚úì Generated: ai_voice_{i:03d}.wav")
                generated_count += 1
                
            except Exception as e:
                print(f"   ‚ùå Error generating voice {i}: {str(e)}")
        
        print(f"\n   Total generated: {generated_count} AI voice samples")
        return generated_count > 0
    
    def load_training_data(self):
        """Load human and AI voice data"""
        print("\n" + "="*70)
        print("üìÇ LOADING TRAINING DATA")
        print("="*70)
        
        X = []
        y = []
        
        # Load human voices (class 0)
        print(f"\nüë§ Loading Human Voices...")
        human_dir = os.path.join(self.data_dir, 'human')
        human_files = list(glob.glob(f"{human_dir}/**/*.wav", recursive=True))
        
        human_loaded = 0
        for filepath in human_files[:100]:  # Limit to 100
            try:
                with open(filepath, 'rb') as f:
                    audio_bytes = f.read()
                
                features = self.audio_processor.extract_features(audio_bytes)
                if features is not None:
                    X.append(list(features.values()))
                    y.append(0)
                    human_loaded += 1
            except:
                pass
        
        print(f"   ‚úì Loaded {human_loaded} human samples")
        
        # Load AI voices (class 1)
        print(f"\nü§ñ Loading AI Voices...")
        ai_files = list(glob.glob(f"{self.data_dir}/**//*.wav", recursive=True))
        ai_files = [f for f in ai_files if 'human' not in f.lower()]
        
        ai_loaded = 0
        for filepath in ai_files[:100]:  # Limit to 100
            try:
                with open(filepath, 'rb') as f:
                    audio_bytes = f.read()
                
                features = self.audio_processor.extract_features(audio_bytes)
                if features is not None:
                    X.append(list(features.values()))
                    y.append(1)
                    ai_loaded += 1
            except:
                pass
        
        print(f"   ‚úì Loaded {ai_loaded} AI samples")
        
        if human_loaded == 0 or ai_loaded == 0:
            print(f"\n‚ùå Insufficient data: Human={human_loaded}, AI={ai_loaded}")
            return None, None
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"\nüìä Dataset: {len(X)} total samples")
        print(f"   Human (0): {np.sum(y == 0)}")
        print(f"   AI (1): {np.sum(y == 1)}")
        
        return X, y
    
    def train_model(self, X, y):
        """Train with better hyperparameters for distinction"""
        print("\n" + "="*70)
        print("ü§ñ TRAINING MODEL WITH DISTINCTION")
        print("="*70)
        
        # Split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Scale
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # Train model with hyperparameters optimized for binary classification
        print(f"\nüå≤ Training RandomForest...")
        print(f"   Parameters: n_estimators=300, max_depth=20, min_samples_split=3")
        
        model = RandomForestClassifier(
            n_estimators=300,
            max_depth=20,
            min_samples_split=3,
            min_samples_leaf=1,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1,
            criterion='gini'
        )
        
        model.fit(X_train_scaled, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"\nüìä Test Accuracy: {accuracy:.2%}")
        
        # Details
        cm = confusion_matrix(y_test, y_pred)
        print(f"\n   Confusion Matrix:")
        print(f"   {cm[0, 0]:3d} Human‚ÜíHuman   {cm[0, 1]:3d} Human‚ÜíAI")
        print(f"   {cm[1, 0]:3d} AI‚ÜíHuman     {cm[1, 1]:3d} AI‚ÜíAI")
        
        # Per-class accuracy
        human_acc = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if (cm[0, 0] + cm[0, 1]) > 0 else 0
        ai_acc = cm[1, 1] / (cm[1, 0] + cm[1, 1]) if (cm[1, 0] + cm[1, 1]) > 0 else 0
        
        print(f"\n   Human Detection Rate: {human_acc:.2%}")
        print(f"   AI Detection Rate: {ai_acc:.2%}")
        
        if accuracy < 0.70:
            print(f"\n‚ö†Ô∏è  WARNING: Model accuracy is only {accuracy:.2%}")
            print(f"   This model may not work well in production")
        
        return model, scaler, accuracy
    
    def save_model(self, model, scaler):
        """Save model"""
        model_path = os.path.join(self.output_dir, 'voice_detection_model.pkl')
        scaler_path = os.path.join(self.output_dir, 'voice_detection_scaler.pkl')
        
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        
        print(f"\nüíæ Model saved!")
        print(f"   {model_path}")
        print(f"   {scaler_path}")
    
    def run(self):
        """Run the fix"""
        try:
            # Try to generate AI voices
            print("\nüöÄ QUICK FIX FOR HUMAN VOICE MISCLASSIFICATION")
            
            has_ai_data = self.generate_ai_voices()
            
            if not has_ai_data:
                print("\n‚ö†Ô∏è  Could not generate AI voices with gTTS")
                print("Attempting to use existing data...")
            
            # Load data
            X, y = self.load_training_data()
            
            if X is None or len(X) < 20:
                print("\n‚ùå Insufficient training data!")
                print("Available data:")
                print(f"   Human voices: Check data/train/human/")
                print(f"   AI voices: Check data/train/ai_generated/")
                return False
            
            # Train
            model, scaler, accuracy = self.train_model(X, y)
            
            # Save
            self.save_model(model, scaler)
            
            print(f"\n" + "="*70)
            if accuracy >= 0.70:
                print(f"‚úÖ FIX COMPLETE - Model accuracy: {accuracy:.2%}")
                print(f"‚úÖ Human voices should now be correctly detected!")
            else:
                print(f"‚ö†Ô∏è  FIX APPLIED - Model accuracy: {accuracy:.2%}")
                print(f"‚ö†Ô∏è  Accuracy is lower than ideal - consider collecting more training data")
            print("="*70)
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

if __name__ == '__main__':
    fixer = QuickModelFixer()
    success = fixer.run()
    sys.exit(0 if success else 1)
